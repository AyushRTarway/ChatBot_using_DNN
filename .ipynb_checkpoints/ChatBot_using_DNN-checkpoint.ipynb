{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee1ba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91878\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2375b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c3173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91878\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn \n",
    "import random \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee4224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Intent.json') as data:\n",
    "    intents = json.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77a8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f39cc532",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = [] \n",
    "documents  = []\n",
    "ignore = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbabc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        \n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        \n",
    "        document.append((w,intent['tag']))\n",
    "        \n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150f0018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'Is',\n",
       " 'anyone',\n",
       " 'there',\n",
       " '?',\n",
       " 'Hello',\n",
       " 'Good',\n",
       " 'day',\n",
       " 'Bye',\n",
       " 'See',\n",
       " 'you',\n",
       " 'later',\n",
       " 'Goodbye',\n",
       " 'Thanks',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'helpful',\n",
       " 'What',\n",
       " 'hours',\n",
       " 'are',\n",
       " 'you',\n",
       " 'open',\n",
       " '?',\n",
       " 'What',\n",
       " 'are',\n",
       " 'your',\n",
       " 'hours',\n",
       " '?',\n",
       " 'When',\n",
       " 'are',\n",
       " 'you',\n",
       " 'open',\n",
       " '?',\n",
       " 'What',\n",
       " 'is',\n",
       " 'your',\n",
       " 'location',\n",
       " '?',\n",
       " 'Where',\n",
       " 'are',\n",
       " 'you',\n",
       " 'located',\n",
       " '?',\n",
       " 'What',\n",
       " 'is',\n",
       " 'your',\n",
       " 'address',\n",
       " '?',\n",
       " 'Where',\n",
       " 'is',\n",
       " 'your',\n",
       " 'restaurant',\n",
       " 'situated',\n",
       " '?',\n",
       " 'Do',\n",
       " 'you',\n",
       " 'take',\n",
       " 'credit',\n",
       " 'cards',\n",
       " '?',\n",
       " 'Do',\n",
       " 'you',\n",
       " 'accept',\n",
       " 'Mastercard',\n",
       " '?',\n",
       " 'Are',\n",
       " 'you',\n",
       " 'cash',\n",
       " 'only',\n",
       " '?',\n",
       " 'What',\n",
       " 'is',\n",
       " 'your',\n",
       " 'menu',\n",
       " 'for',\n",
       " 'today',\n",
       " '?',\n",
       " 'What',\n",
       " 'are',\n",
       " 'you',\n",
       " 'serving',\n",
       " 'today',\n",
       " '?',\n",
       " 'What',\n",
       " 'is',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'special',\n",
       " '?',\n",
       " 'Do',\n",
       " 'you',\n",
       " 'provide',\n",
       " 'home',\n",
       " 'delivery',\n",
       " '?',\n",
       " 'Do',\n",
       " 'you',\n",
       " 'deliver',\n",
       " 'the',\n",
       " 'food',\n",
       " '?',\n",
       " 'What',\n",
       " 'are',\n",
       " 'the',\n",
       " 'home',\n",
       " 'delivery',\n",
       " 'options',\n",
       " '?',\n",
       " 'What',\n",
       " 'is',\n",
       " 'your',\n",
       " 'Menu',\n",
       " '?',\n",
       " 'What',\n",
       " 'are',\n",
       " 'the',\n",
       " 'main',\n",
       " 'course',\n",
       " 'options',\n",
       " '?',\n",
       " 'Can',\n",
       " 'you',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'the',\n",
       " 'most',\n",
       " 'delicious',\n",
       " 'dish',\n",
       " 'from',\n",
       " 'the',\n",
       " 'menu',\n",
       " '?',\n",
       " 'What',\n",
       " 'is',\n",
       " 'the',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'special',\n",
       " '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf52539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [Stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
    "words = sorted(list(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93aa3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(set(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38c2a945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31  documents\n",
      "9  classes\n",
      "58  words\n"
     ]
    }
   ],
   "source": [
    "print(len(document),\" documents\")\n",
    "print(len(classes),\" classes\")\n",
    "print(len(words),\" words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d58a24d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2814573853.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [51]\u001b[1;36m\u001b[0m\n\u001b[1;33m    document[][:]\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c6608b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "output_empty = [0]*len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a6b96fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91878\\AppData\\Local\\Temp\\ipykernel_18084\\3329664506.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "for doc in document:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "      \n",
    "    pattern_words = [Stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag,output_row])\n",
    "    \n",
    "random.shuffle(training)\n",
    "training = np.array(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6bc0417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da828e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(training[:,0])\n",
    "y_train = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec2b4e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91878\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tflearn\\initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None,len(x_train[0])])\n",
    "net = tflearn.fully_connected(net,10)\n",
    "net = tflearn.fully_connected(net,10)\n",
    "net = tflearn.fully_connected(net,len(y_train[0]),activation='softmax')\n",
    "net = tflearn.regression(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92925bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tflearn.DNN(net,tensorboard_dir='tflearn_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d0c16f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 11999  | total loss: \u001b[1m\u001b[32m0.17081\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 3000 | loss: 0.17081 - acc: 0.9856 -- iter: 24/31\n",
      "Training Step: 12000  | total loss: \u001b[1m\u001b[32m0.15856\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 3000 | loss: 0.15856 - acc: 0.9871 -- iter: 31/31\n",
      "--\n",
      "INFO:tensorflow:D:\\PROJECTS\\CHATBOT\\chatmodel.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train,n_epoch = 2000,batch_size = 8 ,show_metric = True)\n",
    "model.save('chatmodel.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8682191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({\n",
    "    'words':words,\n",
    "    'classes':classes,\n",
    "    'X_train':x_train,\n",
    "    'Y_train':y_train\n",
    "},open(\"training_data_structures\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae31bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"training_data_structures\",'rb'))\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "X_train = data['X_train']\n",
    "Y_train = data['Y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67efd841",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Intent.json\") as jdata:\n",
    "    intents = json.load(jdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a5338d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:\\PROJECTS\\CHATBOT\\chatmodel.tflearn\n"
     ]
    }
   ],
   "source": [
    "model.load('./chatmodel.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "382d1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_Up_Sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    sentence_words = [Stemmer.stem(words.lower()) for words in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37e4a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_Of_Words(sentence,words,show_details=False):\n",
    "    \n",
    "    sentence_words = clean_Up_Sentence(sentence)\n",
    "    bag = [0]*len(words)\n",
    "    \n",
    "    for sen in sentence_words:\n",
    "        for index,word in enumerate(words):\n",
    "            if word==sen:\n",
    "                bag[index] = 1\n",
    "                if show_details:\n",
    "                    print(\"word found in the bag : %s\"%word)\n",
    "    \n",
    "    return (np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a7f3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_THRESHOLD = 0.30\n",
    "\n",
    "def classify(sentence):\n",
    "    \n",
    "    results = model.predict([bag_Of_Words(sentence,words)])[0]\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    \n",
    "    results.sort(key=lambda x:x[1],reverse=True)\n",
    "    results_list = []\n",
    "    \n",
    "    for res in results:\n",
    "        results_list.append((classes[res[0]],res[1]))\n",
    "        \n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b38df385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(sentence,userID = '123',show_details=False):\n",
    "    results = classes(sentence)\n",
    "    \n",
    "    if results:\n",
    "        while results:\n",
    "            for ind in intents['intents']:\n",
    "                if ind['tag'] == results[0][0]:\n",
    "                    return print(random.choice(ind['responses']))\n",
    "                \n",
    "            results.pop(0)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ccb01e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hours', 0.8345992)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('What are hours of operation')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "747ba59c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
